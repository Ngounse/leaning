{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a394c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from statsmodels.imputation import mice\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_recall_fscore_support\n",
    "from sklearn.model_selection import learning_curve\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cf93f3a-b58a-44bf-8fab-3cd8d5d0d170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_data = \"handle-missing-values-master/test_dataset/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc915e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read succesfully ! Shape of original file : (891, 11)\n"
     ]
    }
   ],
   "source": [
    "# load and read csv file\n",
    "dframe = pd.read_csv(csv_data).iloc[:, 1:]\n",
    "print('File read succesfully !', f'Shape of original file : {dframe.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11812971-1e86-44dd-858a-fda27f73ef8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2daf8a9-3d39-478b-a8e2-60280b49ffbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Following columns contains missing values : \n",
      "Age : 19.87 %\n",
      "Cabin : 77.10 %\n",
      "Embarked : 0.22 %\n"
     ]
    }
   ],
   "source": [
    "def detect_missing():\n",
    "    # checking missing values\n",
    "    null_series = dframe.isnull().sum()\n",
    "    print()\n",
    "    null_column_list = []\n",
    "    if sum(null_series):\n",
    "        print('Following columns contains missing values : ')\n",
    "        total_samples = dframe.shape[0]\n",
    "        for i, j in null_series.items():\n",
    "            if j:\n",
    "                print(\"{} : {:.2f} %\".format(i, (j/total_samples)*100))\n",
    "                null_column_list.append(i)\n",
    "    else:\n",
    "        print(\"None of the columns contains missing values !\")\n",
    "    return null_column_list\n",
    "    \n",
    "null_column_list = detect_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeaf16a-bb20-45d9-bacd-17f3241fe1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fce4046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using row removal algorithm...\n",
      "Shape of new dataframe : (183, 11)\n",
      "Total 708 rows removed\n"
     ]
    }
   ],
   "source": [
    "# using row removal\n",
    "def row_removal():\n",
    "    original_row, original_col = dframe.shape[0], dframe.shape[1]\n",
    "    print()\n",
    "    print('Using row removal algorithm...')\n",
    "    # removing rows\n",
    "    df_row = dframe.dropna(axis=0)\n",
    "    print(f\"Shape of new dataframe : {df_row.shape}\")\n",
    "    print(f\"Total {original_row - df_row.shape[0]} rows removed\")\n",
    "    return df_row\n",
    "\n",
    "df_row = row_removal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ee8b8-a7d1-41a6-b9a2-96b4eb54424d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52e67e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using column removal algorithm...\n",
      "Warning : Features may be reduced, introducing inconsistency when Testing !\n",
      "Shape of new dataframe : (891, 8)\n",
      "Total 3 columns removed\n"
     ]
    }
   ],
   "source": [
    "# using column removal\n",
    "def column_removal():\n",
    "    original_row, original_col = dframe.shape[0], dframe.shape[1]\n",
    "    print()\n",
    "    print('Using column removal algorithm...')\n",
    "    print('Warning : Features may be reduced, introducing inconsistency when Testing !')\n",
    "    # removing columns\n",
    "    df_col = dframe.dropna(axis=1)\n",
    "    print(f\"Shape of new dataframe : {df_col.shape}\")\n",
    "    print(f\"Total {original_col - df_col.shape[1]} columns removed\")\n",
    "    return df_col\n",
    "\n",
    "df_col = column_removal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4a103-90e0-4968-b271-dcb579d94a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc9c97d3-322a-4466-8665-370407559491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Statistical imputation algorithm...\n",
      "Imputing following columns with mean, median and mode : ['Age']\n",
      "Imputing following columns with mode : ['Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "def stats_imputation( null_column_list):\n",
    "        print()\n",
    "        print('Using Statistical imputation algorithm...')\n",
    "        # extracting columns for numerical columns\n",
    "        valid_cols = [column for column in null_column_list if dframe[column].dtype != 'object']\n",
    "        # extracting columns for categorical columns\n",
    "        categorical_cols = [column for column in null_column_list if dframe[column].dtype == 'object']\n",
    "        numeric_cols = valid_cols\n",
    "        dframe_stats_mean, dframe_stats_median, dframe_stats_mode = dframe.copy(), dframe.copy(), dframe.copy()\n",
    "        # Imputing mean for numeric values and then imputing median and mode for categorical values\n",
    "        print(f'Imputing following columns with mean, median and mode : {numeric_cols}')\n",
    "        print(f'Imputing following columns with mode : {categorical_cols}')\n",
    "        if len(numeric_cols):\n",
    "            for i in numeric_cols:\n",
    "                dframe_stats_mean.fillna({i : dframe[i].mean()}, inplace=True)\n",
    "                dframe_stats_median.fillna({i : dframe[i].median()}, inplace=True)\n",
    "                dframe_stats_mode.fillna({i : dframe[i].mode()[0]}, inplace=True)\n",
    "\n",
    "        if len(categorical_cols):\n",
    "            for i in categorical_cols:\n",
    "                dframe_stats_mean.fillna({i : dframe[i].mode()[0]}, inplace=True)\n",
    "                dframe_stats_median.fillna({i : dframe[i].mode()[0]}, inplace=True)\n",
    "                dframe_stats_mode.fillna({i : dframe[i].mode()[0]}, inplace=True)\n",
    "\n",
    "        return dframe_stats_mean, dframe_stats_median, dframe_stats_mode\n",
    "    \n",
    "df_stats_mean, df_stats_median, df_stats_mode = stats_imputation(null_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc2c51-4793-4099-801e-c81b3e40b625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc544ea-9edf-4250-a24a-5af7d2052f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using interpolation algorithm using linear method...\n"
     ]
    }
   ],
   "source": [
    "# using interpolation algorithm\n",
    "def interpolate_impute():\n",
    "    print()\n",
    "    print('Using interpolation algorithm using linear method...')\n",
    "    df_interpolate = dframe.copy()\n",
    "    # mapping embarked values by numeric values\n",
    "    embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n",
    "    df_interpolate['Embarked'] = df_interpolate['Embarked'].map(embarked_mapping)\n",
    "    # mapping Cabin string values by numeric values\n",
    "    deck = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"U\": 7}\n",
    "    df_interpolate['Cabin'] = df_interpolate['Cabin'].fillna(\"U\")\n",
    "    df_interpolate['Cabin'] = df_interpolate['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    df_interpolate['Cabin'] = df_interpolate['Cabin'].map(deck)\n",
    "    df_interpolate['Cabin'].replace({7:np.nan}, inplace=True)\n",
    "    df_interpolate.interpolate(method='linear', inplace=True, limit_direction='both')\n",
    "    # reverse mapping the values\n",
    "    embarked_mapping = {1:\"S\", 2:\"C\", 3:\"Q\"}\n",
    "    df_interpolate['Embarked'] = df_interpolate['Embarked'].map(embarked_mapping)\n",
    "    deck_mapping = {0 : \"A\", 1 : \"B\", 2 : \"C\", 3 : \"D\", 4 : \"E\", 5 : \"F\", 6 : \"G\"}\n",
    "    df_interpolate['Cabin'] = df_interpolate['Cabin'].map(deck_mapping)\n",
    "    return df_interpolate\n",
    "\n",
    "df_interpolate = interpolate_impute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0619c22c-98d6-48e1-a432-9f6cdaa75bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36268ed3-7dec-4d9f-b11c-f41f8d9ba5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using MICE algorithm...\n",
      "Operating on following features : ['Age', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# using MICE algorithm\n",
    "def MICE_impute():\n",
    "    print()\n",
    "    print('Using MICE algorithm...')\n",
    "    df_mice = dframe.copy()\n",
    "    # mapping Embarked using numeric values\n",
    "    embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n",
    "    df_mice['Embarked'] = df_mice['Embarked'].map(embarked_mapping)\n",
    "    # mapping Cabin using numeric values\n",
    "    deck = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"U\": 7}\n",
    "    df_mice['Cabin'] = df_mice['Cabin'].fillna(\"U\")\n",
    "    df_mice['Cabin'] = df_mice['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    df_mice['Cabin'] = df_mice['Cabin'].map(deck)\n",
    "    df_mice['Cabin'].replace({7:np.nan}, inplace=True)\n",
    "\n",
    "    numeric_features = [column for column in df_mice.columns if df_mice[column].dtype != 'object']\n",
    "    imp = mice.MICEData(df_mice[numeric_features])\n",
    "    imp.set_imputer('')\n",
    "    for i in range(100):\n",
    "        imp.update_all()\n",
    "    operated_cols = [column for column in numeric_features if dframe[column].isnull().sum()]\n",
    "    print(f'Operating on following features : {operated_cols}')\n",
    "    # copying the imputed values to the original df\n",
    "    for i in operated_cols:\n",
    "        df_mice[i] = imp.data[i]\n",
    "\n",
    "    # reverse mapping the values\n",
    "    embarked_mapping = {1:\"S\", 2:\"C\", 3:\"Q\"}\n",
    "    df_mice['Embarked'] = df_mice['Embarked'].map(embarked_mapping)\n",
    "    deck_mapping = {0 : \"A\", 1 : \"B\", 2 : \"C\", 3 : \"D\", 4 : \"E\", 5 : \"F\", 6 : \"G\"}\n",
    "    df_mice['Cabin'] = df_mice['Cabin'].map(deck_mapping)\n",
    "    return df_mice\n",
    "\n",
    "df_mice = MICE_impute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc030c33-8e43-449f-90e0-86c5309c4a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29ed59-90dd-476e-9d18-a3fd85a627ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c1148-6dff-4a3a-aa1a-8822b93be90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "858ec644-0259-47a9-bdc0-7acc517c0320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessing the dataset\n",
    "def preprocess( df_train):\n",
    "    print()\n",
    "    # dropping the features\n",
    "    fields_to_drop = ['Name', 'Ticket']\n",
    "    df_train = df_train.drop(fields_to_drop, axis=1)\n",
    "    dummy_fields = [column for column in df_train.columns if df_train[column].dtype == 'object']\n",
    "    # concatenating the categorical features\n",
    "    for each in dummy_fields:\n",
    "        dummies = pd.get_dummies(df_train[each], drop_first=False)\n",
    "        df_train = pd.concat([df_train, dummies], axis=1)\n",
    "        df_train = df_train.drop([each], axis=1)\n",
    "\n",
    "    return df_train\n",
    "\n",
    "# splitting the dataset\n",
    "def split_dataset( df_train):\n",
    "    # X is training features, y is labels\n",
    "    X = df_train.iloc[:, 1:]\n",
    "    y = df_train.iloc[:, 0]\n",
    "    return train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# function to predict the values after fitting the model\n",
    "# Here, i have chosen to use LogisticRegression model as a baseline\n",
    "# so that , effect of different algorithms can be seen clearly.\n",
    "def predict( x_train, x_test, y_train, y_test):\n",
    "    logreg = LogisticRegression(solver='lbfgs', max_iter=700, random_state=0)\n",
    "    logreg.fit(x_train, y_train)\n",
    "    logreg_predictions = logreg.predict(x_test)\n",
    "    return logreg_predictions\n",
    "\n",
    "# function to evaluate model and print the metrics\n",
    "def evaluate( y_pred, y_test):\n",
    "    print(f\"Accuracy : {round(accuracy_score(y_test, y_pred) * 100, 2)}\")\n",
    "    print(f\"Log_loss : {log_loss(y_test, y_pred)}\")\n",
    "    precision, recall = precision_recall_fscore_support(y_test, y_pred)[0], precision_recall_fscore_support(y_test, y_pred)[1]\n",
    "    print(f\"precision : {precision} , recall : {recall}\")\n",
    "\n",
    "# plot cross validation scores for all the algorithms\n",
    "def plot_metrics( df_list):\n",
    "    # df_list is a tuple of all the x_train and x_test\n",
    "    test_scores_mean = []\n",
    "    for df in df_list:\n",
    "        estimator = LogisticRegression(solver='lbfgs', max_iter=700, random_state=0)\n",
    "        train_sizes, train_scores, test_scores = learning_curve(estimator, df[0], df[1], cv=5, random_state=0)\n",
    "        test_scores_mean.append(np.mean(test_scores, axis=1))\n",
    "\n",
    "    print(\"Plotting final metrics cross validation scores for all algorithms : \")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Cross-validation score\")\n",
    "    plt.title('LOGISTIC REGRESSION ALGORITHM')\n",
    "    plt.plot(train_sizes, test_scores_mean[0], 'o-', color=\"b\", label=\"row removal\")\n",
    "    plt.plot(train_sizes, test_scores_mean[1], 'o-', color=\"g\", label=\"column removal\")\n",
    "    plt.plot(train_sizes, test_scores_mean[2], 'o-', color=\"k\", label=\"mean imputed\")\n",
    "    plt.plot(train_sizes, test_scores_mean[3], 'o-', color=\"c\", label=\"median imputed\")\n",
    "    plt.plot(train_sizes, test_scores_mean[4], 'o-', color=\"m\", label=\"mode imputed\")\n",
    "    plt.plot(train_sizes, test_scores_mean[5], 'o-', color=\"y\", label=\"interpolation imputed\")\n",
    "    plt.plot(train_sizes, test_scores_mean[6], 'o-', color=\"r\", label=\"MICE imputed\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be601b-86ca-4dfa-94ea-52b2d45379dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4fc8a-c282-4eb2-ba3d-f4e73e324361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for row removal algorithm\n",
      "\n",
      "Accuracy : 78.18\n",
      "Log_loss : 7.535805722292348\n",
      "precision : [0.53333333 0.875     ] , recall : [0.61538462 0.83333333]\n",
      "\n",
      "Metrics for column removal algorithm\n",
      "\n",
      "Accuracy : 79.48\n",
      "Log_loss : 7.088253328576164\n",
      "precision : [0.82285714 0.74193548] , recall : [0.85714286 0.69      ]\n",
      "\n",
      "Metrics for mean imputation algorithm\n",
      "\n",
      "Accuracy : 81.34\n",
      "Log_loss : 6.4438791390997\n",
      "precision : [0.85542169 0.74509804] , recall : [0.8452381 0.76     ]\n",
      "\n",
      "Metrics for median imputation algorithm\n",
      "\n",
      "Accuracy : 81.34\n",
      "Log_loss : 6.4438791390997\n",
      "precision : [0.85542169 0.74509804] , recall : [0.8452381 0.76     ]\n",
      "\n",
      "Metrics for mode imputation algorithm\n",
      "\n",
      "Accuracy : 81.34\n",
      "Log_loss : 6.443876155527198\n",
      "precision : [0.85119048 0.75      ] , recall : [0.85119048 0.75      ]\n",
      "\n",
      "Metrics for interpolation imputation algorithm\n",
      "\n",
      "Accuracy : 79.85\n",
      "Log_loss : 6.959383264396873\n",
      "precision : [0.83529412 0.73469388] , recall : [0.8452381 0.72     ]\n",
      "\n",
      "Metrics for MICE imputation algorithm\n",
      "\n",
      "Accuracy : 80.6\n",
      "Log_loss : 6.701625234603284\n",
      "precision : [0.8372093 0.75     ] , recall : [0.85714286 0.72      ]\n",
      "Maximum Accuracy and minimum loss is obtained from using MICE imputation algorithm !\n"
     ]
    }
   ],
   "source": [
    "# list to be stored as tupled train, test features for plotting cross validation scores\n",
    "features_training_list = []\n",
    "# metrics modelling for row removal algorithm\n",
    "print()\n",
    "print(\"Metrics for row removal algorithm\")\n",
    "df_train = preprocess(df_row)\n",
    "x_train, x_test, y_train, y_test = split_dataset(df_train)\n",
    "y_pred = predict(x_train, x_test, y_train, y_test)\n",
    "evaluate(y_pred, y_test)\n",
    "features_training_list.append((x_train, y_train))\n",
    "# metrics column removal\n",
    "print()\n",
    "print(\"Metrics for column removal algorithm\")\n",
    "df_train = preprocess(df_col)\n",
    "x_train, x_test, y_train, y_test = split_dataset(df_train)\n",
    "y_pred = predict(x_train, x_test, y_train, y_test)\n",
    "evaluate(y_pred, y_test)\n",
    "features_training_list.append((x_train, y_train))\n",
    "# metrics mean imputation\n",
    "print()\n",
    "print(\"Metrics for mean imputation algorithm\")\n",
    "df_train = preprocess(df_stats_mean)\n",
    "x_train, x_test, y_train, y_test = split_dataset(df_train)\n",
    "y_pred = predict(x_train, x_test, y_train, y_test)\n",
    "evaluate(y_pred, y_test)\n",
    "features_training_list.append((x_train, y_train))\n",
    "# metrics median imputation\n",
    "print()\n",
    "print(\"Metrics for median imputation algorithm\")\n",
    "df_train = preprocess(df_stats_median)\n",
    "x_train, x_test, y_train, y_test = split_dataset(df_train)\n",
    "y_pred = predict(x_train, x_test, y_train, y_test)\n",
    "evaluate(y_pred, y_test)\n",
    "features_training_list.append((x_train, y_train))\n",
    "# metrics mode imputation\n",
    "print()\n",
    "print(\"Metrics for mode imputation algorithm\")\n",
    "df_train = preprocess(df_stats_mode)\n",
    "x_train, x_test, y_train, y_test = split_dataset(df_train)\n",
    "y_pred = predict(x_train, x_test, y_train, y_test)\n",
    "evaluate(y_pred, y_test)\n",
    "features_training_list.append((x_train, y_train))\n",
    "# metrics interpolatation algorithm\n",
    "print()\n",
    "print(\"Metrics for interpolation imputation algorithm\")\n",
    "df_train = preprocess(df_interpolate)\n",
    "x_train, x_test, y_train, y_test = split_dataset(df_train)\n",
    "y_pred = predict(x_train, x_test, y_train, y_test)\n",
    "evaluate(y_pred, y_test)\n",
    "features_training_list.append((x_train, y_train))\n",
    "# metrics MICE algorithm\n",
    "print()\n",
    "print(\"Metrics for MICE imputation algorithm\")\n",
    "df_train = preprocess(df_mice)\n",
    "x_train, x_test, y_train, y_test = split_dataset(df_train)\n",
    "y_pred = predict(x_train, x_test, y_train, y_test)\n",
    "evaluate(y_pred, y_test)\n",
    "features_training_list.append((x_train, y_train))\n",
    "\n",
    "print(\"Maximum Accuracy and minimum loss is obtained from using MICE imputation algorithm !\")\n",
    "plot_metrics(features_training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daeff9c-22ae-4f75-acce-204093d4a26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43202e-32c1-4fb4-bf75-db85b26e491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file and writing to output path provided\n",
    "df_mice.to_csv(\"data/output.csv\", index = False)\n",
    "print()\n",
    "print(f\"DataFrame of shape {df_mice.shape} written to data/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936ae1a-c218-42f3-b0e6-38147e504758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ded5f9-21dc-4071-82bb-8644f3443d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful when , you need to see the actual Statistics of missing values\n",
    "# Plotting results for best missing values handling algorithm\n",
    "# # plotting results\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(30,30))\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.4)\n",
    "sns.heatmap(dframe.isnull(), yticklabels=False, cbar=False, cmap='rocket', ax = axes[0][0]).set_title(\"Original\")\n",
    "sns.heatmap(df_row.isnull(), yticklabels=False, cbar=False, cmap='rocket', ax = axes[0][1]).set_title(\"Row removal\")\n",
    "sns.heatmap(df_col.isnull(), yticklabels=False, cbar=False, cmap='rocket', ax = axes[0][2]).set_title(\"Column_removal\")\n",
    "sns.heatmap(df_stats_mean.isnull(), yticklabels=False, cbar=False, cmap='rocket', ax = axes[0][3]).set_title(\"Mean imputation\")\n",
    "sns.heatmap(df_stats_median.isnull(), yticklabels=False, cbar=False, cmap='rocket', ax = axes[1][0]).set_title(\"Median imputation\")\n",
    "sns.heatmap(df_stats_mode.isnull(), yticklabels=False, cbar=False, cmap='rocket', ax = axes[1][1]).set_title(\"Mode imputation\")\n",
    "sns.heatmap(df_interpolate.isnull(), yticklabels=False, cbar=False, cmap='rocket', ax = axes[1][2]).set_title(\"interpolation imputation\")\n",
    "sns.heatmap(df_mice.isnull(), yticklabels=False, cbar=False, cmap='rocket', ax = axes[1][3]).set_title(\"MICE imputation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbaaffb-a263-4621-9728-f3676d283090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb825f57-4ec2-420d-a0ec-ac0bf92b97ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca86ce-cfc0-4d0d-8de0-2fa8ee5828c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
